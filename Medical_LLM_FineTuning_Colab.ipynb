{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wasxy47/Medical_LLM_FineTuning_Colab/blob/main/Medical_LLM_FineTuning_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4n4aDEbhW6m"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q unsloth\n",
        "!pip install -q transformers datasets accelerate bitsandbytes\n",
        "!pip install -q trl peft torch"
      ],
      "metadata": {
        "id": "i_6HFSi7huvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from datasets import load_dataset\n"
      ],
      "metadata": {
        "id": "ABSz3p8Qh39m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = 2048,  # You can reduce this if you get memory errors\n",
        "    load_in_4bit = True,\n",
        "    device_map = \"auto\", # Explicitly set device_map to 'auto'\n",
        "    # token = \"hf_...\", # Add your HuggingFace token if needed\n",
        ")"
      ],
      "metadata": {
        "id": "9yDWE5IdiOuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"medalpaca/medical_meadow_medical_flashcards\")"
      ],
      "metadata": {
        "id": "MnvWkc8Viltz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the above doesn't work, we'll create a simple medical dataset\n",
        "medical_data = {\n",
        "    \"instruction\": [\n",
        "        \"What are the symptoms of diabetes?\",\n",
        "        \"How is hypertension treated?\",\n",
        "        \"What causes asthma attacks?\",\n",
        "        \"Describe the treatment for bacterial pneumonia\",\n",
        "    ],\n",
        "    \"input\": [\"\"] * 4,  # Empty input\n",
        "    \"output\": [\n",
        "        \"Common symptoms of diabetes include frequent urination, excessive thirst, extreme hunger, unexplained weight loss, fatigue, blurred vision, and slow-healing sores.\",\n",
        "        \"Hypertension is typically treated with lifestyle modifications including reduced salt intake, regular exercise, weight management, and medications like ACE inhibitors, beta-blockers, or diuretics.\",\n",
        "        \"Asthma attacks can be triggered by allergens like pollen and dust, respiratory infections, cold air, exercise, stress, air pollutants, and certain medications.\",\n",
        "        \"Bacterial pneumonia is treated with antibiotics targeting the specific bacteria, along with supportive care including rest, hydration, and fever-reducing medications. Severe cases may require hospitalization.\",\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "WX4qmzI_iYdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "dataset = Dataset.from_dict(medical_data)"
      ],
      "metadata": {
        "id": "aMWIKGDKir_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add LoRA adapters to the model for efficient fine-tuning\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,  # Rank of LoRA adaptation\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = True,\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "QAaAorxyivwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure training parameters\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"medical-model\",     # Where to save the model\n",
        "    per_device_train_batch_size = 2,  # Reduce if you get memory errors\n",
        "    gradient_accumulation_steps = 4,  # Accumulate gradients\n",
        "    warmup_steps = 5,                 # Learning rate warmup\n",
        "    num_train_epochs = 3,             # Number of training cycles\n",
        "    learning_rate = 2e-4,             # Learning rate\n",
        "    fp16 = not torch.cuda.is_bf16_supported(),  # Use mixed precision\n",
        "    bf16 = torch.cuda.is_bf16_supported(),\n",
        "    logging_steps = 1,                # Log progress\n",
        "    optim = \"adamw_8bit\",             # Optimizer\n",
        "    weight_decay = 0.01,              # Regularization\n",
        "    lr_scheduler_type = \"linear\",     # Learning rate schedule\n",
        "    seed = 3407,                      # Random seed\n",
        "    report_to = \"none\",               # Disable external logging\n",
        ")"
      ],
      "metadata": {
        "id": "vaYqsgCai1YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_instruction_examples(example):\n",
        "    prompt = f\"### Human: {example['instruction']}\\n### Assistant:\"\n",
        "    answer = example['output']\n",
        "    return [f\"{prompt} {answer}\"]\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    formatting_func=format_instruction_examples,  # returns list of strings\n",
        "    max_seq_length=1024,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "h8CnJBSLi69Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monitor GPU memory usage\n",
        "!pip install -q GPUtil\n",
        "import GPUtil\n",
        "GPUtil.showUtilization()\n",
        "\n",
        "# Or use this for detailed monitoring\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "snm5qTaIi-L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"medical_lora_adapter\")  # Saves only the adapter\n",
        "tokenizer.save_pretrained(\"medical_lora_adapter\")\n",
        "\n",
        "# model.push_to_hub(\"your-username/medical-llama-3\")\n",
        "# tokenizer.push_to_hub(\"your-username/medical-llama-3\")"
      ],
      "metadata": {
        "id": "k8-EV8UPjAex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with medical questions\n",
        "questions = [\n",
        "    \"What are common symptoms of heart attack?\",\n",
        "    \"How is diabetes diagnosed?\",\n",
        "    \"What is the treatment for migraine?\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    prompt = f\"### Human: {question}\\n### Assistant:\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=150,   # only generate new content\n",
        "        do_sample=True,       # makes output more natural\n",
        "        temperature=0.7,      # controls randomness\n",
        "        top_p=0.9,            # nucleus sampling\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # Remove the prompt from output\n",
        "    answer = answer.replace(prompt, \"\").strip()\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"A: {answer}\\n\")"
      ],
      "metadata": {
        "id": "BwDeJKwdjF6w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}